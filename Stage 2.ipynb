{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted files to C:\\Users\\SATHWIK\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Specify the name of the uploaded zip file\n",
    "uploaded_zip_file = 'deu-eng.zip'\n",
    "\n",
    "# Specify the directory where you want to extract the contents\n",
    "extracted_dir = 'C:\\\\Users\\\\SATHWIK'\n",
    "\n",
    "# Create the target directory if it doesn't exist\n",
    "if not os.path.exists(extracted_dir):\n",
    "    os.makedirs(extracted_dir)\n",
    "\n",
    "# Open the uploaded zip file\n",
    "zip_file_path = os.path.join(os.getcwd(), uploaded_zip_file)\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    # Extract all the contents into the target directory\n",
    "    zip_ref.extractall(extracted_dir)\n",
    "\n",
    "print(f\"Successfully extracted files to {extracted_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a loaded document into pairs\n",
    "def to_pairs(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in lines]\n",
    "    return pairs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "\n",
    "# clean a list of lines\n",
    "def clean_pairs(lines):\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            # normalize unicode characters\n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            # tokenize on white space\n",
    "            line = line.split()\n",
    "            # convert to lowercase\n",
    "            line = [word.lower() for word in line]\n",
    "            # remove punctuation from each token\n",
    "            line = [re_punc.sub('', w) for w in line]\n",
    "            # remove non-printable chars form each token\n",
    "            line = [re_print.sub('', w) for w in line]\n",
    "            # remove tokens with numbers in them\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            # store as string\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return array(cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-german.pkl\n",
      "[go] => [geh]\n",
      "[hi] => [hallo]\n",
      "[hi] => [gru gott]\n",
      "[run] => [lauf]\n",
      "[run] => [lauf]\n",
      "[wow] => [potzdonner]\n",
      "[wow] => [donnerwetter]\n",
      "[duck] => [kopf runter]\n",
      "[fire] => [feuer]\n",
      "[help] => [hilfe]\n",
      "[help] => [zu hulf]\n",
      "[stay] => [bleib]\n",
      "[stop] => [stopp]\n",
      "[stop] => [anhalten]\n",
      "[wait] => [warte]\n",
      "[wait] => [warte]\n",
      "[begin] => [fang an]\n",
      "[do it] => [mache es]\n",
      "[do it] => [tue es]\n",
      "[go on] => [mach weiter]\n",
      "[hello] => [hallo]\n",
      "[hello] => [sers]\n",
      "[hello] => [hallo]\n",
      "[hurry] => [beeil dich]\n",
      "[hurry] => [schnell]\n",
      "[i hid] => [ich versteckte mich]\n",
      "[i hid] => [ich habe mich versteckt]\n",
      "[i ran] => [ich rannte]\n",
      "[i see] => [ich verstehe]\n",
      "[i see] => [aha]\n",
      "[i try] => [ich versuche es]\n",
      "[i try] => [ich probiere es]\n",
      "[i won] => [ich hab gewonnen]\n",
      "[i won] => [ich habe gewonnen]\n",
      "[i won] => [ich habe gewonnen]\n",
      "[oh no] => [oh nein]\n",
      "[relax] => [entspann dich]\n",
      "[shoot] => [feuer]\n",
      "[shoot] => [schie]\n",
      "[smile] => [lacheln]\n",
      "[sorry] => [entschuldigung]\n",
      "[ask me] => [frag mich]\n",
      "[ask me] => [fragt mich]\n",
      "[ask me] => [fragen sie mich]\n",
      "[attack] => [angriff]\n",
      "[attack] => [attacke]\n",
      "[buy it] => [kaufs]\n",
      "[cheers] => [zum wohl]\n",
      "[eat it] => [iss es]\n",
      "[eat up] => [iss fertig]\n",
      "[eat up] => [iss auf]\n",
      "[eat up] => [iss auf]\n",
      "[exhale] => [ausatmen]\n",
      "[freeze] => [keine bewegung]\n",
      "[freeze] => [stehenbleiben]\n",
      "[go now] => [geh jetzt]\n",
      "[got it] => [verstanden]\n",
      "[got it] => [ich habs]\n",
      "[got it] => [aha]\n",
      "[got it] => [kapiert]\n",
      "[got it] => [verstanden]\n",
      "[got it] => [einverstanden]\n",
      "[he ran] => [er rannte]\n",
      "[he ran] => [er lief]\n",
      "[hop in] => [mach mit]\n",
      "[hop in] => [spring rein]\n",
      "[hug me] => [druck mich]\n",
      "[hug me] => [nimm mich in den arm]\n",
      "[hug me] => [umarme mich]\n",
      "[i care] => [mir ist es wichtig]\n",
      "[i fell] => [ich fiel]\n",
      "[i fell] => [ich fiel hin]\n",
      "[i fell] => [ich sturzte]\n",
      "[i fell] => [ich bin hingefallen]\n",
      "[i fell] => [ich bin gesturzt]\n",
      "[i fled] => [ich fluchtete]\n",
      "[i fled] => [ich bin gefluchtet]\n",
      "[i know] => [ich wei]\n",
      "[i lied] => [ich habe gelogen]\n",
      "[i lost] => [ich habe verloren]\n",
      "[i paid] => [ich habe bezahlt]\n",
      "[i paid] => [ich zahlte]\n",
      "[i pass] => [ich passe]\n",
      "[i sang] => [ich sang]\n",
      "[i spit] => [ich spuckte]\n",
      "[i spit] => [ich habe gespuckt]\n",
      "[i swim] => [ich schwimme]\n",
      "[i wept] => [ich weinte]\n",
      "[i wept] => [ich habe geweint]\n",
      "[im] => [ich bin jahre alt]\n",
      "[im] => [ich bin]\n",
      "[im ok] => [mir gehts gut]\n",
      "[im ok] => [es geht mir gut]\n",
      "[im up] => [ich bin wach]\n",
      "[im up] => [ich bin auf]\n",
      "[inhale] => [einatmen]\n",
      "[listen] => [hort zu]\n",
      "[no way] => [unmoglich]\n",
      "[no way] => [das kommt nicht in frage]\n",
      "[no way] => [das gibts doch nicht]\n"
     ]
    }
   ],
   "source": [
    "from pickle import dump  # Assuming you are using pickle for serialization\n",
    "\n",
    "# ...\n",
    "\n",
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)\n",
    "\n",
    "# load dataset\n",
    "filename = 'deu.txt'\n",
    "doc = load_doc(filename)\n",
    "# split into English-German pairs\n",
    "pairs = to_pairs(doc)\n",
    "# clean sentences\n",
    "cleaned_pairs = clean_pairs(pairs)\n",
    "# save clean pairs to file\n",
    "save_clean_data(cleaned_pairs, 'english-german.pkl')\n",
    "# spot check\n",
    "for i in range(100):\n",
    "    print('[%s] => [%s]' % (cleaned_pairs[i, 0], cleaned_pairs[i, 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-german-both.pkl\n",
      "Saved: english-german-train.pkl\n",
      "Saved: english-german-test.pkl\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from pickle import dump\n",
    "from numpy.random import shuffle\n",
    "\n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)\n",
    "\n",
    "# load dataset\n",
    "raw_dataset = load_clean_sentences('english-german.pkl')\n",
    "\n",
    "# reduce dataset size\n",
    "n_sentences = 10000\n",
    "dataset = raw_dataset[:n_sentences, :]\n",
    "\n",
    "# random shuffle\n",
    "shuffle(dataset)\n",
    "\n",
    "# split into train/test\n",
    "train, test = dataset[:9000], dataset[9000:]\n",
    "\n",
    "# save\n",
    "save_clean_data(dataset, 'english-german-both.pkl')\n",
    "save_clean_data(train, 'english-german-train.pkl')\n",
    "save_clean_data(test, 'english-german-test.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "# load datasets\n",
    "dataset = load_clean_sentences('english-german-both.pkl')\n",
    "train = load_clean_sentences('english-german-train.pkl')\n",
    "test = load_clean_sentences('english-german-test.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (23.3.1)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (2.13.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.13.0 in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (20.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (4.25.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (50.3.1.post20201107)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.15.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.11.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.59.3)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow) (0.35.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.5.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.13.0->tensorflow) (2.4.7)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (7.0.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\sathwik\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "# Define and fit the tokenizer on your data\n",
    "# Define and fit the tokenizer on your data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max sentence length\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 2178\n",
      "English Max Length: 5\n",
      "German Vocabulary Size: 3552\n",
      "German Max Length: 8\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "dataset = load_clean_sentences('english-german-both.pkl')\n",
    "train = load_clean_sentences('english-german-train.pkl')\n",
    "test = load_clean_sentences('english-german-test.pkl')\n",
    "\n",
    "# Prepare English tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "\n",
    "# Check if the tokenizer is created successfully\n",
    "if eng_tokenizer is not None and eng_tokenizer.word_index:\n",
    "    eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "    eng_length = max_length(dataset[:, 0])\n",
    "    print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "    print('English Max Length: %d' % eng_length)\n",
    "\n",
    "    # Prepare German tokenizer\n",
    "    ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "    \n",
    "    # Check if the German tokenizer is created successfully\n",
    "    if ger_tokenizer is not None and ger_tokenizer.word_index:\n",
    "        ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "        ger_length = max_length(dataset[:, 1])\n",
    "        print('German Vocabulary Size: %d' % ger_vocab_size)\n",
    "        print('German Max Length: %d' % ger_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=10, padding='post')\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode target sequence\n",
    "def encode_output(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = array(ylist)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "trainY = encode_output(trainY, eng_vocab_size)\n",
    "\n",
    "# Prepare validation data\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "testY = encode_output(testY, eng_vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define NMT model\n",
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(n_units))\n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "    model.add(LSTM(n_units, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    \n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 10, 256)           909312    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 256)               525312    \n",
      "                                                                 \n",
      " repeat_vector_1 (RepeatVec  (None, 10, 256)           0         \n",
      " tor)                                                            \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 10, 256)           525312    \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDi  (None, 10, 2178)          559746    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2519682 (9.61 MB)\n",
      "Trainable params: 2519682 (9.61 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.70889, saving model to model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SATHWIK\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 - 33s - loss: 2.4598 - val_loss: 1.7089 - 33s/epoch - 233ms/step\n",
      "Epoch 2/50\n",
      "\n",
      "Epoch 2: val_loss improved from 1.70889 to 1.60139, saving model to model.h5\n",
      "141/141 - 25s - loss: 1.6397 - val_loss: 1.6014 - 25s/epoch - 180ms/step\n",
      "Epoch 3/50\n",
      "\n",
      "Epoch 3: val_loss improved from 1.60139 to 1.55370, saving model to model.h5\n",
      "141/141 - 24s - loss: 1.5596 - val_loss: 1.5537 - 24s/epoch - 173ms/step\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 4: val_loss improved from 1.55370 to 1.52286, saving model to model.h5\n",
      "141/141 - 24s - loss: 1.5081 - val_loss: 1.5229 - 24s/epoch - 173ms/step\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 5: val_loss improved from 1.52286 to 1.46555, saving model to model.h5\n",
      "141/141 - 25s - loss: 1.4547 - val_loss: 1.4655 - 25s/epoch - 175ms/step\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 6: val_loss improved from 1.46555 to 1.42857, saving model to model.h5\n",
      "141/141 - 24s - loss: 1.3897 - val_loss: 1.4286 - 24s/epoch - 174ms/step\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 7: val_loss improved from 1.42857 to 1.39162, saving model to model.h5\n",
      "141/141 - 24s - loss: 1.3286 - val_loss: 1.3916 - 24s/epoch - 173ms/step\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 8: val_loss improved from 1.39162 to 1.32792, saving model to model.h5\n",
      "141/141 - 24s - loss: 1.2562 - val_loss: 1.3279 - 24s/epoch - 170ms/step\n",
      "Epoch 9/50\n",
      "\n",
      "Epoch 9: val_loss improved from 1.32792 to 1.27620, saving model to model.h5\n",
      "141/141 - 25s - loss: 1.1862 - val_loss: 1.2762 - 25s/epoch - 177ms/step\n",
      "Epoch 10/50\n",
      "\n",
      "Epoch 10: val_loss improved from 1.27620 to 1.23434, saving model to model.h5\n",
      "141/141 - 24s - loss: 1.1177 - val_loss: 1.2343 - 24s/epoch - 174ms/step\n",
      "Epoch 11/50\n",
      "\n",
      "Epoch 11: val_loss improved from 1.23434 to 1.18849, saving model to model.h5\n",
      "141/141 - 24s - loss: 1.0505 - val_loss: 1.1885 - 24s/epoch - 174ms/step\n",
      "Epoch 12/50\n",
      "\n",
      "Epoch 12: val_loss improved from 1.18849 to 1.15233, saving model to model.h5\n",
      "141/141 - 25s - loss: 0.9853 - val_loss: 1.1523 - 25s/epoch - 177ms/step\n",
      "Epoch 13/50\n",
      "\n",
      "Epoch 13: val_loss improved from 1.15233 to 1.11804, saving model to model.h5\n",
      "141/141 - 24s - loss: 0.9248 - val_loss: 1.1180 - 24s/epoch - 171ms/step\n",
      "Epoch 14/50\n",
      "\n",
      "Epoch 14: val_loss improved from 1.11804 to 1.08289, saving model to model.h5\n",
      "141/141 - 24s - loss: 0.8688 - val_loss: 1.0829 - 24s/epoch - 171ms/step\n",
      "Epoch 15/50\n",
      "\n",
      "Epoch 15: val_loss improved from 1.08289 to 1.06375, saving model to model.h5\n",
      "141/141 - 24s - loss: 0.8148 - val_loss: 1.0638 - 24s/epoch - 173ms/step\n",
      "Epoch 16/50\n",
      "\n",
      "Epoch 16: val_loss improved from 1.06375 to 1.04335, saving model to model.h5\n",
      "141/141 - 25s - loss: 0.7618 - val_loss: 1.0433 - 25s/epoch - 177ms/step\n",
      "Epoch 17/50\n",
      "\n",
      "Epoch 17: val_loss improved from 1.04335 to 1.00963, saving model to model.h5\n",
      "141/141 - 24s - loss: 0.7101 - val_loss: 1.0096 - 24s/epoch - 171ms/step\n",
      "Epoch 18/50\n",
      "\n",
      "Epoch 18: val_loss improved from 1.00963 to 0.99753, saving model to model.h5\n",
      "141/141 - 24s - loss: 0.6621 - val_loss: 0.9975 - 24s/epoch - 171ms/step\n",
      "Epoch 19/50\n",
      "\n",
      "Epoch 19: val_loss improved from 0.99753 to 0.98146, saving model to model.h5\n",
      "141/141 - 24s - loss: 0.6148 - val_loss: 0.9815 - 24s/epoch - 171ms/step\n",
      "Epoch 20/50\n",
      "\n",
      "Epoch 20: val_loss improved from 0.98146 to 0.95934, saving model to model.h5\n",
      "141/141 - 24s - loss: 0.5692 - val_loss: 0.9593 - 24s/epoch - 170ms/step\n",
      "Epoch 21/50\n",
      "\n",
      "Epoch 21: val_loss improved from 0.95934 to 0.94447, saving model to model.h5\n",
      "141/141 - 25s - loss: 0.5243 - val_loss: 0.9445 - 25s/epoch - 174ms/step\n",
      "Epoch 22/50\n",
      "\n",
      "Epoch 22: val_loss improved from 0.94447 to 0.92821, saving model to model.h5\n",
      "141/141 - 24s - loss: 0.4837 - val_loss: 0.9282 - 24s/epoch - 174ms/step\n",
      "Epoch 23/50\n",
      "\n",
      "Epoch 23: val_loss improved from 0.92821 to 0.91624, saving model to model.h5\n",
      "141/141 - 24s - loss: 0.4436 - val_loss: 0.9162 - 24s/epoch - 174ms/step\n",
      "Epoch 24/50\n",
      "\n",
      "Epoch 24: val_loss improved from 0.91624 to 0.91081, saving model to model.h5\n",
      "141/141 - 24s - loss: 0.4068 - val_loss: 0.9108 - 24s/epoch - 173ms/step\n",
      "Epoch 25/50\n",
      "\n",
      "Epoch 25: val_loss improved from 0.91081 to 0.89780, saving model to model.h5\n",
      "141/141 - 24s - loss: 0.3726 - val_loss: 0.8978 - 24s/epoch - 173ms/step\n",
      "Epoch 26/50\n",
      "\n",
      "Epoch 26: val_loss improved from 0.89780 to 0.89389, saving model to model.h5\n",
      "141/141 - 24s - loss: 0.3407 - val_loss: 0.8939 - 24s/epoch - 173ms/step\n",
      "Epoch 27/50\n",
      "\n",
      "Epoch 27: val_loss improved from 0.89389 to 0.88790, saving model to model.h5\n",
      "141/141 - 24s - loss: 0.3120 - val_loss: 0.8879 - 24s/epoch - 173ms/step\n",
      "Epoch 28/50\n",
      "\n",
      "Epoch 28: val_loss improved from 0.88790 to 0.88616, saving model to model.h5\n",
      "141/141 - 26s - loss: 0.2824 - val_loss: 0.8862 - 26s/epoch - 187ms/step\n",
      "Epoch 29/50\n",
      "\n",
      "Epoch 29: val_loss improved from 0.88616 to 0.88291, saving model to model.h5\n",
      "141/141 - 25s - loss: 0.2589 - val_loss: 0.8829 - 25s/epoch - 175ms/step\n",
      "Epoch 30/50\n",
      "\n",
      "Epoch 30: val_loss improved from 0.88291 to 0.87841, saving model to model.h5\n",
      "141/141 - 25s - loss: 0.2381 - val_loss: 0.8784 - 25s/epoch - 174ms/step\n",
      "Epoch 31/50\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.87841\n",
      "141/141 - 24s - loss: 0.2164 - val_loss: 0.8790 - 24s/epoch - 173ms/step\n",
      "Epoch 32/50\n",
      "\n",
      "Epoch 32: val_loss improved from 0.87841 to 0.87198, saving model to model.h5\n",
      "141/141 - 25s - loss: 0.1975 - val_loss: 0.8720 - 25s/epoch - 174ms/step\n",
      "Epoch 33/50\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.87198\n",
      "141/141 - 25s - loss: 0.1814 - val_loss: 0.8772 - 25s/epoch - 174ms/step\n",
      "Epoch 34/50\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.87198\n",
      "141/141 - 25s - loss: 0.1663 - val_loss: 0.8760 - 25s/epoch - 174ms/step\n",
      "Epoch 35/50\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.87198\n",
      "141/141 - 24s - loss: 0.1534 - val_loss: 0.8751 - 24s/epoch - 174ms/step\n",
      "Epoch 36/50\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.87198\n",
      "141/141 - 25s - loss: 0.1413 - val_loss: 0.8806 - 25s/epoch - 174ms/step\n",
      "Epoch 37/50\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.87198\n",
      "141/141 - 25s - loss: 0.1318 - val_loss: 0.8743 - 25s/epoch - 174ms/step\n",
      "Epoch 38/50\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.87198\n",
      "141/141 - 24s - loss: 0.1224 - val_loss: 0.8801 - 24s/epoch - 174ms/step\n",
      "Epoch 39/50\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.87198\n",
      "141/141 - 24s - loss: 0.1127 - val_loss: 0.8784 - 24s/epoch - 174ms/step\n",
      "Epoch 40/50\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.87198\n",
      "141/141 - 25s - loss: 0.1055 - val_loss: 0.8887 - 25s/epoch - 175ms/step\n",
      "Epoch 41/50\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.87198\n",
      "141/141 - 27s - loss: 0.0996 - val_loss: 0.8890 - 27s/epoch - 190ms/step\n",
      "Epoch 42/50\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.87198\n",
      "141/141 - 25s - loss: 0.0937 - val_loss: 0.8890 - 25s/epoch - 174ms/step\n",
      "Epoch 43/50\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.87198\n",
      "141/141 - 24s - loss: 0.0883 - val_loss: 0.8919 - 24s/epoch - 172ms/step\n",
      "Epoch 44/50\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.87198\n",
      "141/141 - 24s - loss: 0.0845 - val_loss: 0.8912 - 24s/epoch - 173ms/step\n",
      "Epoch 45/50\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.87198\n",
      "141/141 - 24s - loss: 0.0796 - val_loss: 0.8972 - 24s/epoch - 173ms/step\n",
      "Epoch 46/50\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.87198\n",
      "141/141 - 26s - loss: 0.0763 - val_loss: 0.9110 - 26s/epoch - 181ms/step\n",
      "Epoch 47/50\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.87198\n",
      "141/141 - 24s - loss: 0.0737 - val_loss: 0.9028 - 24s/epoch - 173ms/step\n",
      "Epoch 48/50\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.87198\n",
      "141/141 - 25s - loss: 0.0706 - val_loss: 0.9089 - 25s/epoch - 175ms/step\n",
      "Epoch 49/50\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.87198\n",
      "141/141 - 27s - loss: 0.0678 - val_loss: 0.9097 - 27s/epoch - 189ms/step\n",
      "Epoch 50/50\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.87198\n",
      "141/141 - 26s - loss: 0.0644 - val_loss: 0.9165 - 26s/epoch - 186ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1e784f552e0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, RepeatVector, TimeDistributed, Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# Define the encode_sequences and encode_output functions\n",
    "# ...\n",
    "\n",
    "# Define the NMT model\n",
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(n_units))\n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "    model.add(LSTM(n_units, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    \n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Placeholder values, replace them with your actual values\n",
    "src_vocab = 3552\n",
    "tar_vocab = 2178\n",
    "src_timesteps = 10\n",
    "tar_timesteps = 10\n",
    "n_units = 256\n",
    "\n",
    "# Assuming you have previously defined and compiled the model using define_model function\n",
    "model = define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units)\n",
    "\n",
    "# Assuming you have your dataset defined, tokenizers created, and functions defined\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "trainY = encode_output(trainY, eng_vocab_size)\n",
    "\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "testY = encode_output(testY, eng_vocab_size)\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Assuming your sequences are stored in trainX and trainY\n",
    "trainX_padded = pad_sequences(trainX, maxlen=10, padding='post')\n",
    "trainY_padded = pad_sequences(trainY, maxlen=10, padding='post')\n",
    "\n",
    "\n",
    "# Fit model\n",
    "checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(trainX, trainY, epochs=50, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "dataset = load_clean_sentences('english-german-both.pkl')\n",
    "train = load_clean_sentences('english-german-train.pkl')\n",
    "test = load_clean_sentences('english-german-test.pkl')\n",
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "# prepare data\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Load the model\n",
    "model = load_model('model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate target given source sequence\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    integers = [argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = word_for_id(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return ' '.join(target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src=[warte mal kurz], target=[wait a second], predicted=[wait a moment]\n",
      "src=[geh ran tom], target=[get to it tom], predicted=[get to it tom]\n",
      "src=[niemand hat gelogen], target=[nobody lied], predicted=[nobody lied]\n",
      "src=[mach das aus], target=[turn it off], predicted=[turn it on]\n",
      "src=[wir sind im krieg], target=[were at war], predicted=[were are at war]\n",
      "src=[es ist deins], target=[its yours], predicted=[its yours]\n",
      "src=[sie ist sauber], target=[its clean], predicted=[its clean]\n",
      "src=[tom hat eine glatze], target=[tom is bald], predicted=[tom is bald]\n",
      "src=[kannst du es machen], target=[can you do it], predicted=[can you do it]\n",
      "src=[was war das], target=[what was that], predicted=[what was that]\n",
      "BLEU-1: 0.939041\n",
      "BLEU-2: 0.877616\n",
      "BLEU-3: 0.832699\n",
      "BLEU-4: 0.764016\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from numpy import argmax\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from collections import Counter\n",
    "\n",
    "# ...\n",
    "\n",
    "# Evaluate the skill of the model\n",
    "def evaluate_model(model, tokenizer, source, raw_dataset, max_length):\n",
    "    actual, predicted = list(), list()\n",
    "\n",
    "    # Handle non-numeric elements in the sequences\n",
    "    replacement_value = 0\n",
    "    numeric_sequences = [\n",
    "        [int(str(num)) if str(num).isdigit() else replacement_value for num in seq] for seq in source\n",
    "    ]\n",
    "\n",
    "    # Pad all sequences\n",
    "    padded_sequences = pad_sequences(numeric_sequences, maxlen=10, padding='post')\n",
    "\n",
    "    for i, source_seq in enumerate(padded_sequences):\n",
    "        # translate encoded source text\n",
    "        source_seq = source_seq.reshape((1, source_seq.shape[0]))\n",
    "        translation = predict_sequence(model, tokenizer, source_seq)\n",
    "        raw_target, raw_src, additional_info = raw_dataset[i]\n",
    "        if i < 10:\n",
    "            print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "        actual.append(raw_target.split())\n",
    "        predicted.append(translation.split())\n",
    "\n",
    "    # flatten the lists of lists into a single list of strings\n",
    "    actual_flat = [word for sublist in actual for word in sublist]\n",
    "    predicted_flat = [word for sublist in predicted for word in sublist]\n",
    "\n",
    "    # calculate BLEU score with smoothing\n",
    "    smoothing_function = SmoothingFunction().method1\n",
    "    bleu_1 = sentence_bleu([actual_flat], predicted_flat, weights=(1.0, 0, 0, 0), smoothing_function=smoothing_function)\n",
    "    bleu_2 = sentence_bleu([actual_flat], predicted_flat, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing_function)\n",
    "    bleu_3 = sentence_bleu([actual_flat], predicted_flat, weights=(0.3, 0.3, 0.3, 0), smoothing_function=smoothing_function)\n",
    "    bleu_4 = sentence_bleu([actual_flat], predicted_flat, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing_function)\n",
    "\n",
    "    # print BLEU scores\n",
    "    print('BLEU-1: %f' % bleu_1)\n",
    "    print('BLEU-2: %f' % bleu_2)\n",
    "    print('BLEU-3: %f' % bleu_3)\n",
    "    print('BLEU-4: %f' % bleu_4)\n",
    "\n",
    "# Assuming you have defined max_length somewhere before calling this function\n",
    "\n",
    "# Call the evaluate_model function with the model, tokenizer, sources, raw_dataset, and max_length\n",
    "evaluate_model(model, eng_tokenizer, trainX, raw_dataset, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_clean_sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-aac83b51e42a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# load datasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_clean_sentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english-german-both.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_clean_sentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english-german-train.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_clean_sentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english-german-test.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_clean_sentences' is not defined"
     ]
    }
   ],
   "source": [
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# load datasets\n",
    "dataset = load_clean_sentences('english-german-both.pkl')\n",
    "train = load_clean_sentences('english-german-train.pkl')\n",
    "test = load_clean_sentences('english-german-test.pkl')\n",
    "\n",
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "\n",
    "if eng_tokenizer is not None and eng_tokenizer.word_index:\n",
    "    eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "    eng_length = max_length(dataset[:, 0])\n",
    "\n",
    "    # prepare german tokenizer\n",
    "    ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "    ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "    ger_length = max_length(dataset[:, 1])\n",
    "\n",
    "    # prepare data\n",
    "    trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "    testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    "\n",
    "    # load model\n",
    "    model = load_model('model.h5')\n",
    "\n",
    "    # test on some training sequences\n",
    "    print('train')\n",
    "    evaluate_model(model, eng_tokenizer, trainX, train, raw_dataset)\n",
    "\n",
    "    # test on some test sequences\n",
    "    print('test')\n",
    "    evaluate_model(model, eng_tokenizer, testX, test, raw_dataset)\n",
    "else:\n",
    "    print(\"Error: The English tokenizer was not created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
